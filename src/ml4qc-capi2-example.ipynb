{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using the ml4qc package for a specific example analysis (\"CAPI2\")\n",
    "\n",
    "This workbook demonstrates an example analysis for a particular CAPI survey example, using data collected with [SurveyCTO](https://www.surveycto.com). To preserve project confidentiality, the project is simply referred to as \"CAPI2\".\n",
    "\n",
    "This is a basic proof-of-concept project designed to explore the potential for ML-based tools to identify outliers and submissions that are likely to be rejected by a human reviewer. The project collected a wide range of metadata, and many submissions were reviewed.\n",
    "\n",
    "**This workbook is a work in progress...**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data from SurveyCTO export\n",
    "\n",
    "Here, we use the [surveydata](https://surveydata.readthedocs.io/) package to load the data and prep it for analysis. This includes loading and then processing all [text audits](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zd.field-types-text-audit.html), converting from the original long format (one `.csv` file per submission) to a wide format that we merge into the main dataset. In addition to the text audit files, the main .csv file includes the following columns:\n",
    "\n",
    "* **SubmissionDate** - Date submission received by SurveyCTO server\n",
    "* **starttime** - Device date and time form was first opened to fill out\n",
    "* **endtime** - Device date and time form was completed\n",
    "* **mean_light_level** - Mean light level during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html)) (unfortunately, 3/4 of submissions are missing data for this field)\n",
    "* **sd_light_level** - Standard deviation of light level during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html)) (unfortunately, 3/4 of submissions are missing data for this field)\n",
    "* **mean_movement** - Mean device movement during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html)) (unfortunately, 2/3 of submissions are missing data for this field)\n",
    "* **sd_movement** - Standard deviation of device movement during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html)) (unfortunately, 2/3 of submissions are missing data for this field)\n",
    "* **mean_movement** - Mean sound level during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html))\n",
    "* **sd_movement** - Standard deviation of sound level during interview ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html))\n",
    "* **pct_quiet** - Percent of form-editing time when it was quiet ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html))\n",
    "* **pct_still** - Percent of form-editing time when the device was still ([sensor metadata](https://docs.surveycto.com/02-designing-forms/01-core-concepts/03zf.field-types-sensor-statistic.html))\n",
    "* **duration** - Total number of seconds spent editing form\n",
    "* **TA** - Name of text audit `.csv` file\n",
    "* **review_status** - Current review status (APPROVED, REJECTED, or NONE; part of the [review and correction workflow](https://docs.surveycto.com/04-monitoring-and-management/01-the-basics/04.reviewing-and-correcting.html))\n",
    "* **review_quality** - Reviewed submission quality (GOOD, OKAY, POOR, or FAKE; part of the [review and correction workflow](https://docs.surveycto.com/04-monitoring-and-management/01-the-basics/04.reviewing-and-correcting.html))\n",
    "* **KEY** - Unique submission ID"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from surveydata import SurveyCTOPlatform\n",
    "from surveydata import SurveyCTOExportStorage\n",
    "\n",
    "# manually set our data and collection timezones\n",
    "# (needed in this case because the storage and text audit formats used don't themselves include timezone info)\n",
    "data_tz = pytz.timezone(\"Asia/Kabul\").zone\n",
    "collection_tz = pytz.timezone(\"Asia/Kabul\").zone\n",
    "\n",
    "# initialize local storage with .csv file and attachments_available=True since media subdirectory is present\n",
    "storage = SurveyCTOExportStorage(export_file=os.path.expanduser(\"~/ml4qc-data/collab2/capi2/capi2.csv\"), attachments_available=True)\n",
    "\n",
    "# load all submissions into DataFrame and describe contents\n",
    "submissions_df = SurveyCTOPlatform.get_submissions_df(storage)\n",
    "print(\"Submission DataFrame field counts:\")\n",
    "print(submissions_df.count(0))\n",
    "print()\n",
    "\n",
    "# summarize submission review status and quality\n",
    "print(\"Submission DataFrame review status and quality:\")\n",
    "print(submissions_df.review_status.value_counts())\n",
    "print(submissions_df.review_quality.value_counts())\n",
    "print()\n",
    "\n",
    "# load all text audits into DataFrame and describe contents\n",
    "textaudit_df = SurveyCTOPlatform.get_text_audit_df(storage, location_strings=submissions_df.TA)\n",
    "if textaudit_df is not None:\n",
    "    print(\"Text audit DataFrame field counts:\")\n",
    "    print(textaudit_df.count(0))\n",
    "    print()\n",
    "\n",
    "    # summarize text audits in wide format\n",
    "    ta_summary = SurveyCTOPlatform.process_text_audits(textaudit_df, submissions_df[\"starttime\"], submissions_df[\"endtime\"], data_tz, collection_tz)\n",
    "\n",
    "    # merge text wide-format audit summaries with submission data\n",
    "    all_data = pd.concat([submissions_df, ta_summary], axis='columns', join='outer', verify_integrity=True)\n",
    "\n",
    "    # print summary of combined DataFrame\n",
    "    print(\"Combined DataFrame field counts:\")\n",
    "    print(all_data.count(0))\n",
    "else:\n",
    "    print(\"No text audits found.\")\n",
    "\n",
    "    all_data = submissions_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepping data for analysis\n",
    "\n",
    "Here, we organize our features and the classes that we'll be working to predict, and we preprocess the data to prepare for binary classification prediction using \"submissions that are not APPROVED as GOOD quality\" (i.e., rejected or another quality) as the target.\n",
    "\n",
    "Here, we use PCA to reduce our feature dimensions, targeting 99% variance retention. This allows ML model training and validation to be much faster.\n",
    "\n",
    "Since this is a static *ex post* dataset with all submissions already reviewed, we use 75% of the data for training and 25% for testing (our prediction set)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ml4qc import SurveyMLClassifier\n",
    "\n",
    "# optional dimensionality reduction with PCA (None for no PCA, float (0, 1) for % variance to retain\n",
    "pca = 0.99\n",
    "\n",
    "# random state for reproducible results (or None)\n",
    "random_state = 411\n",
    "\n",
    "# make a copy of the data for analysis, so that we can transform it at will, dropping columns we don't need and rows with missing values\n",
    "analysis_df = all_data.drop([\"SubmissionDate\", \"starttime\", \"endtime\", \"duration\", \"TA\", \"mean_light_level\", \"sd_light_level\", \"mean_movement\", \"sd_movement\"], axis=1).copy(deep=True)\n",
    "analysis_df = analysis_df[~analysis_df.isnull().any(axis=1)]\n",
    "\n",
    "# organize DataFrames with features and classes for prediction\n",
    "classes_df = analysis_df.loc[:, [\"review_status\", \"review_quality\"]]\n",
    "features_df = analysis_df.drop([\"review_status\", \"review_quality\"], axis=1)\n",
    "\n",
    "# set our target for prediction\n",
    "target_description = \"Not APPROVED+GOOD\"\n",
    "classes_df[\"target\"] = classes_df.apply(lambda row: (0 if row[\"review_status\"] == \"APPROVED\" and row[\"review_quality\"] == \"GOOD\" else 1), axis=1)\n",
    "target_df = pd.DataFrame(classes_df[\"target\"])\n",
    "print(f\"Target description: {target_description}\")\n",
    "print(target_df.target.value_counts())\n",
    "print()\n",
    "\n",
    "# create SurveyML object with 25% test vs. train size\n",
    "surveyml = SurveyMLClassifier(features_df, target_df, test_size=0.25, cv_when_training=True, random_state=random_state, verbose=True, reweight_classes=True, n_jobs=-2)\n",
    "\n",
    "# preprocess data\n",
    "surveyml.preprocess_for_prediction(pca=pca)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing analysis\n",
    "\n",
    "In the cells that follow, we perform a series of analyses using the `scikit-learn` and `Tensorflow Keras` libraries, making use of the utilities available in the `SurveyMLClassifier` class. These models have not been extensively tuned, and they currently require re-tuning whenever the target or dimensionality-reduction settings change above.\n",
    "\n",
    "### Identifying outliers\n",
    "\n",
    "First, however, we use the `identify_outliers()` method available in the base `SurveyML` class, to identify outliers and save them to disk."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"Isolation forest outlier detection\"\n",
    "print(f\"       Model: {model_description}\")\n",
    "print()\n",
    "\n",
    "# identify outliers\n",
    "x_outlier_df = surveyml.identify_outliers()\n",
    "\n",
    "# save results to .csv\n",
    "x_outlier_df.to_csv(os.path.expanduser(\"~/ml4qc-data/collab2/capi2/capi2_outlier_predictions.csv\"))\n",
    "\n",
    "# combine outlier prediction with target classification and show crosstab\n",
    "x_outlier_df = x_outlier_df.join(target_df)\n",
    "pd.crosstab(x_outlier_df.is_outlier, x_outlier_df.target).apply(lambda c: c/c.sum(), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification prediction with logistic regression\n",
    "\n",
    "Here, we use logistic regression to predict the classification defined during the preprocessing stage above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"Logistic regression\"\n",
    "model_scoring = 'f1'  # 'average_precision' to optimize for precision, 'f1' to optimize for both precision and recall\n",
    "print(f\"  Predicting: {target_description}\")\n",
    "print(f\"       Model: {model_description}\")\n",
    "print(f\"     Scoring: {model_scoring}\")\n",
    "print()\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# first use cross validation to find the optimal hyperparameters\n",
    "classifier_lr = LogisticRegression(random_state=random_state, max_iter=2000)\n",
    "search_params = {'C': skl.utils.fixes.loguniform(0.0001, 100), 'penalty': ['l2'], 'class_weight': ['balanced', None]}\n",
    "params = surveyml.cv_for_best_hyperparameters(classifier=classifier_lr, search_params=search_params, model_scoring=model_scoring, n_iter=100)\n",
    "\n",
    "# then fit the model using the optimal hyperparameters\n",
    "classifier_lr = LogisticRegression(random_state=random_state, max_iter=10000, **params)\n",
    "predictions = surveyml.run_prediction_model(classifier_lr)\n",
    "print(f\"Predictions made: {len(predictions)}\")\n",
    "# surveyml.report_feature_importance(classifier_lr.coef_[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification prediction with a decision tree\n",
    "\n",
    "Here, we use a decision tree to predict the classification defined during the preprocessing stage above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"Decision tree\"\n",
    "model_scoring = 'f1'  # 'average_precision' to optimize for precision, 'f1' to optimize for both precision and recall\n",
    "print(f\"  Predicting: {target_description}\")\n",
    "print(f\"       Model: {model_description}\")\n",
    "print(f\"     Scoring: {model_scoring}\")\n",
    "print()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# first use cross validation to find the optimal hyperparameters\n",
    "classifier_dt = DecisionTreeClassifier(random_state=random_state)\n",
    "search_params = {'max_features': [None, 'sqrt', 'log2'], 'max_depth': [2, 3, 4, 5, 7, 10, 15, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'class_weight': [None, 'balanced']}\n",
    "params = surveyml.cv_for_best_hyperparameters(classifier=classifier_dt, search_params=search_params, model_scoring=model_scoring, n_iter=100)\n",
    "\n",
    "# then fit the model using the optimal hyperparameters\n",
    "classifier_dt = DecisionTreeClassifier(random_state=random_state, **params)\n",
    "predictions = surveyml.run_prediction_model(classifier_dt)\n",
    "print(f\"Predictions made: {len(predictions)}\")\n",
    "# surveyml.report_feature_importance(classifier_dt.feature_importances_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification prediction with random forest\n",
    "\n",
    "Here, we use random forest to predict the classification defined during the preprocessing stage above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"Random forest\"\n",
    "model_scoring = 'f1'  # 'average_precision' to optimize for precision, 'f1' to optimize for both precision and recall\n",
    "print(f\"  Predicting: {target_description}\")\n",
    "print(f\"       Model: {model_description}\")\n",
    "print(f\"     Scoring: {model_scoring}\")\n",
    "print()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# first use cross validation to find the optimal hyperparameters\n",
    "classifier_rf = RandomForestClassifier(random_state=random_state, n_estimators=50)\n",
    "search_params = {'max_features': [None, 'sqrt', 'log2'], 'max_depth': [2, 3, 4, 5, 7, 10, 15, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'class_weight': [None, 'balanced'], 'bootstrap': [True]}\n",
    "params = surveyml.cv_for_best_hyperparameters(classifier=classifier_rf, search_params=search_params, model_scoring=model_scoring, n_iter=100)\n",
    "\n",
    "# then fit the model using the optimal hyperparameters\n",
    "classifier_rf = RandomForestClassifier(random_state=random_state, n_estimators=500, **params)\n",
    "predictions = surveyml.run_prediction_model(classifier_rf)\n",
    "print(f\"Predictions made: {len(predictions)}\")\n",
    "# surveyml.report_feature_importance(classifier_rf.feature_importances_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification prediction with XGBoost\n",
    "\n",
    "Here, we use the XGBoost algorithm to predict the classification defined during the preprocessing stage above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"XGBoost\"\n",
    "model_scoring = 'f1'  # 'average_precision' to optimize for precision, 'f1' to optimize for both precision and recall\n",
    "print(f\"  Predicting: {target_description}\")\n",
    "print(f\"       Model: {model_description}\")\n",
    "print(f\"     Scoring: {model_scoring}\")\n",
    "print()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# first use cross validation to find the optimal hyperparameters\n",
    "classifier_xb = XGBClassifier(random_state=random_state, objective='binary:logistic', eval_metric='aucpr', n_estimators=50)\n",
    "search_params = {'max_depth': [2, 3, 4, 5, 7, 10], 'min_child_weight': [1, 2, 4], 'subsample': [0.5, 0.75, 1], 'scale_pos_weight': [1, surveyml.neg_train/surveyml.pos_train]}\n",
    "params = surveyml.cv_for_best_hyperparameters(classifier=classifier_xb, search_params=search_params, model_scoring=model_scoring, n_iter=100)\n",
    "\n",
    "# then fit the model using the optimal hyperparameters\n",
    "classifier_xb = XGBClassifier(random_state=random_state, objective='binary:logistic', eval_metric='aucpr', n_estimators=500, **params)\n",
    "predictions = surveyml.run_prediction_model(classifier_xb)\n",
    "print(f\"Predictions made: {len(predictions)}\")\n",
    "# surveyml.report_feature_importance(classifier_xb.feature_importances_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification prediction with a neural network\n",
    "\n",
    "Here, we use a neural network to predict the classification defined during the preprocessing stage above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_description = \"Neural network\"\n",
    "model_scoring = 'f1'  # 'average_precision' to optimize for precision, 'f1' to optimize for both precision and recall\n",
    "print(f\"  Predicting: {target_description}\")\n",
    "print(f\"       Model: {model_description}\")\n",
    "print(f\"     Scoring: {model_scoring}\")\n",
    "print()\n",
    "\n",
    "import numpy as np\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# first use cross validation to find the optimal hyperparameters\n",
    "import sklearn as skl\n",
    "classifier_nn = KerasClassifier(random_state=random_state, model=surveyml.build_nn_model, verbose=0)\n",
    "#   test initial bias to match imbalance in classes (from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)\n",
    "search_params = {'epochs': [10, 20, 40, 80, 100], 'batch_size': [16, 32], 'class_weight': [None, surveyml.class_weights], 'model__features': [surveyml.num_features], 'model__hidden_layers': [1, 2], 'model__initial_units': [surveyml.num_features, surveyml.num_features / 2], 'model__activation': ['relu', 'sigmoid'], 'model__l2_regularization': [True, False], 'model__l2_factor': skl.utils.fixes.loguniform(0.00001, 0.1), 'model__include_dropout': [True, False], 'model__dropout_rate': np.linspace(start=0.05, stop=0.5, num=10), 'model__output_bias': [None, np.log([surveyml.pos_train/surveyml.neg_train])]}\n",
    "params = surveyml.cv_for_best_hyperparameters(classifier=classifier_nn, search_params=search_params, model_scoring=model_scoring, n_iter=100)\n",
    "\n",
    "# then fit the model using the optimal hyperparameters\n",
    "classifier_nn = KerasClassifier(random_state=random_state, model=surveyml.build_nn_model, verbose=0, **params)\n",
    "predictions = surveyml.run_prediction_model(classifier_nn, supports_cv=True)\n",
    "print(classifier_nn.model_.summary())\n",
    "print()\n",
    "print(f\"Predictions made: {len(predictions)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
